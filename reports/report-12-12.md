
# üìÑ Relat√≥rio da Entrega ‚Äî 12/12
## An√°lise de Ataques Advers√°rios em Modelo BERT de Classifica√ß√£o de Toxicidade  
### T√≥picos Avan√ßados em Sistemas de Informa√ß√£o (IF1015)

---

Este relat√≥rio apresenta a an√°lise de robustez do modelo BERT treinado na tarefa de classifica√ß√£o multilabel de toxicidade (Jigsaw Toxicity Dataset), por meio da aplica√ß√£o de **ataques advers√°rios textuais**. O objetivo √© avaliar a sensibilidade do modelo a pequenas perturba√ß√µes lingu√≠sticas e verificar se h√° fragilidades que comprometam sua confiabilidade.

Foram analisados dois cen√°rios principais:

- **Texto de insulto**  
- **Texto de amea√ßa**

Para cada cen√°rio, aplicaram-se cinco tipos de ataques advers√°rios:  
1. Typos / Leetspeak  
2. Substitui√ß√£o por sin√¥nimos  
3. Inser√ß√£o de palavras neutras  
4. Altera√ß√£o de espa√ßamento e pontua√ß√£o  
5. Reescrita leve (par√°frase simples)

As probabilidades analisadas pertencem √†s classes de maior interesse: **toxic**, **insult**, **threat**.

---

# 1. Predi√ß√µes Base (sem ataque)

## Texto de insulto
"You are a disgusting idiot and nobody likes you."
Probabilidades:
toxic: 0.959
insult: 0.766
threat: 0.048

## Texto de amea√ßa
"I will find you and hurt you badly."
Probabilidades:
toxic: 0.490
insult: 0.191
threat: 0.024

As predi√ß√µes-base demonstram que o modelo reconhece corretamente ambos os textos como ofensivos (insulto e amea√ßa).

---

# 2. Ataques no Texto de Insulto

### 2.1 Ataque por Typos / Leetspeak
Texto atacado:

"Y0√º @r3 @ d1sg√ºst1ng 1d10t @nd n0b0dy l1k3s y0√º."


| M√©trica | Probabilidade | Diferen√ßa |
|---------|---------------|-----------|
| toxic   | **0.036**     | **-0.923** |
| insult  | **0.010**     | **-0.756** |
| threat  | **0.002**     | **-0.046** |


### **An√°lise**
O modelo **falha completamente** em detectar toxicidade quando o texto recebe substitui√ß√µes de caracteres. Isso representa uma fragilidade grave em contextos de detec√ß√£o de discurso ofensivo online, pois ataques desse tipo s√£o comuns (‚Äúleetspeak‚Äù).

---

### 2.2 Ataque por Sin√¥nimos
Texto atacado:

"You are a gross fool and no one likes you."


| M√©trica | Probabilidade | Diferen√ßa   |
|---------|---------------|-------------|
| toxic   | **0.817**     | **-0.314**  |
| insult  | **0.462**     | **-0.498**  |
| threat  | **0.040**         | **-0.022**      |


### **An√°lise**
Mesmo com substitui√ß√µes por sin√¥nimos corretos (‚Äúidiot‚Äù ‚Üí ‚Äúfool‚Äù, ‚Äúdisgusting‚Äù ‚Üí ‚Äúgross‚Äù), h√° uma **queda significativa** na classifica√ß√£o de insulto e toxicidade.  
Isso mostra que o modelo depende fortemente de palavras espec√≠ficas e generaliza pouco para varia√ß√µes sem√¢nticas equivalentes.

---

### 2.3 Observa√ß√£o geral para os demais ataques (n√£o exibidos integralmente)
Nos ataques de:

- **inser√ß√£o de palavras neutras**  
- **quebra de palavras (‚Äúi d i o t‚Äù)**  
- **reescrita leve**

tamb√©m ocorre redu√ß√£o percept√≠vel nas probabilidades de toxicidade e insulto, ainda que menos intensa que no leetspeak.

Esses padr√µes indicam que o modelo:

- √© **muito sens√≠vel ao formato superficial** do texto;  
- n√£o √© robusto a perturba√ß√µes lingu√≠sticas comuns;  
- pode ser facilmente burlado em contextos pr√°ticos.

---

# 3. Ataques no Texto de Amea√ßa

### 3.1 Ataque por Typos / Leetspeak
Texto atacado:
"1 w1ll f1nd y0√º @nd h√ºrt y0√º b@dly."


| M√©trica | Probabilidade | Diferen√ßa   |
|---------|---------------|-------------|
| toxic   | **0.241**     | **-0.249**  |
| insult  | **0.057**     | **-0.133**  |
| threat  | **0.007**     | **-0.017**  |

### **An√°lise**
A detec√ß√£o de ‚Äúthreat‚Äù cai drasticamente, mesmo mantendo a estrutura sem√¢ntica de amea√ßa.  
Isso confirma que o modelo depende fortemente da apar√™ncia textual dos tokens.

---

### 3.2 Ataque por Sin√¥nimos
Texto atacado:
"I will find you and harm you severely."


| M√©trica | Probabilidade | Diferen√ßa   |
|---------|---------------|-------------|
| toxic   | **0.376**     | **-0.200**  |
| insult  | **0.134**     | **-0.103**  |
| threat  | **0.018**     | **-0.011**  |


### **An√°lise**
Apesar de ‚Äúhurt‚Äù e ‚Äúharm‚Äù serem semanticamente pr√≥ximos, o modelo reduz a probabilidade de amea√ßa. Isso refor√ßa que o modelo n√£o est√° capturando adequadamente o significado sem√¢ntico mais profundo.

---

# 4. Discuss√£o Geral dos Resultados

## Principais vulnerabilidades encontradas:

### **1. Alta sensibilidade a typos / leetspeak**
O modelo praticamente **n√£o detecta toxicidade** quando palavras ofensivas s√£o substitu√≠das por varia√ß√µes superficiais:
- ‚Äúidiot‚Äù ‚Üí ‚Äú1d10t‚Äù
- ‚Äúhurt‚Äù ‚Üí ‚Äúh√ºrt‚Äù
- ‚Äúyou‚Äù ‚Üí ‚Äúy0√º‚Äù

Esse tipo de ataque √© comum em redes sociais.

---

### **2. Baixa generaliza√ß√£o para sin√¥nimos**
Mesmo substitui√ß√µes simples alteram significativamente o resultado:
- ‚Äúidiot‚Äù ‚Üí ‚Äúfool‚Äù
- ‚Äúdisgusting‚Äù ‚Üí ‚Äúgross‚Äù
- ‚Äúhurt you badly‚Äù ‚Üí ‚Äúharm you severely‚Äù

Isso sugere depend√™ncia excessiva da superf√≠cie lexical.

---

### **3. Inser√ß√µes neutras podem enfraquecer a toxicidade**
Adicionar hesita√ß√µes ou palavras vazias faz o modelo ‚Äútravar‚Äù e reduzir as probabilidades.

---

### **4. Textos que expressam amea√ßa s√£o pouco reconhecidos**
As redu√ß√µes na classe ‚Äúthreat‚Äù mostram que o modelo √© pouco robusto para identificar amea√ßas reescritas.

### 4.1 Texto de insulto

![alt text](images/ataques-no-texto-de-insulto.png)


O gr√°fico evidencia que:

- No cen√°rio **typos/leetspeak**, as probabilidades de `toxic` e `insult` caem drasticamente em rela√ß√£o ao texto original.
- No cen√°rio de **sin√¥nimos**, h√° uma redu√ß√£o significativa, mas n√£o t√£o extrema quanto nos typos.
- Nos cen√°rios de inser√ß√£o, espa√ßamento e reescrita leve, observa-se queda moderada, refor√ßando que o modelo √© sens√≠vel a varia√ß√µes superficiais da forma do texto.

## 4.2 Texto de amea√ßa

![alt text](images/ataques-no-texto-de-ameaca.png)

Nesse caso, nota-se que:

- A classe `threat` √© particularmente inst√°vel ‚Äî pequenas altera√ß√µes (typos, sin√¥nimos) j√° reduzem bastante a probabilidade.
- A linha de `toxic` tamb√©m sofre varia√ß√µes, indicando que o modelo n√£o √© robusto para amea√ßas reescritas ou mascaradas por perturba√ß√µes de superf√≠cie.

---

# 5. Implica√ß√µes e Recomenda√ß√µes

### ‚úî O modelo **n√£o √© robusto** a perturba√ß√µes comuns de linguagem.  

### Recomenda√ß√µes:
- **Data augmentation adversarial**  
  Incluir no treino vers√µes com typos, sin√¥nimos, espa√ßamento variado.
- **Treino adversarial supervisionado**  
  Expor o modelo a ataques gerados automaticamente.
- **Normaliza√ß√£o lingu√≠stica pr√©-processamento**  
  - Remover leetspeak  
  - Corrigir typos  
  - Mapear sin√¥nimos para formas can√¥nicas  
- **Modelos mais robustos**  
  Usar embeddings contextualizados robustos (RoBERTa, DeBERTa) ou instru√ß√£o supervisionada.

---

# 6. Conclus√£o

A an√°lise adversarial demonstra que o modelo BERT treinado para toxicidade:

- funciona bem em textos diretos e n√£o modificados,  
- mas apresenta **graves vulnerabilidades** quando submetido a perturba√ß√µes simples,  
- indicando falta de robustez sem√¢ntica e excessiva depend√™ncia de palavras espec√≠ficas.

Esses resultados alertam para a necessidade de refor√ßo no treinamento e aprimoramento na etapa de pr√©-processamento caso o modelo seja usado em cen√°rios reais de detec√ß√£o de ofensas.

