# üìù Relat√≥rio ‚Äî An√°lise Parcial + Robustez do Modelo  
## Projeto Final ‚Äî IF1015 (T√≥picos Avan√ßados em SI 6)  
### Classifica√ß√£o de Toxicidade usando BERT ‚Äî Baseline + Robustez

---

# 1. Introdu√ß√£o

Este relat√≥rio apresenta o desenvolvimento e avalia√ß√£o inicial de um modelo de classifica√ß√£o de toxicidade em coment√°rios online utilizando t√©cnicas de Processamento de Linguagem Natural (PLN) e Aprendizagem Profunda.  

A entrega √© dividida em duas partes:

- **Treinamento Parcial (21/11)** ‚Äî defini√ß√£o da aplica√ß√£o, dataset, modelo e resultados iniciais.  
- **An√°lise de Robustez (28/11)** ‚Äî avalia√ß√£o da estabilidade do modelo frente a perturba√ß√µes lingu√≠sticas.

---

# 2. Defini√ß√£o da Aplica√ß√£o

A tarefa consiste em prever m√∫ltiplos r√≥tulos de toxicidade em coment√°rios textuais. O modelo deve classificar cada coment√°rio nas categorias:

- toxic  
- severe_toxic  
- obscene  
- threat  
- insult  
- identity_hate  

Essa tarefa √© essencial em modera√ß√£o autom√°tica de conte√∫do, seguran√ßa digital e prote√ß√£o de usu√°rios. Al√©m disso, serve de base para estudos de robustez, interpretabilidade e ataques adversariais.

---

# 3. Dataset

### Fonte  
**Jigsaw Toxic Comment Classification Challenge** ‚Äî Kaggle

### Formato  
CSV contendo coluna `comment_text` e seis r√≥tulos bin√°rios.

### Tamanho original  
159.571 exemplos.

### Subset usado  
Para viabilizar a execu√ß√£o em tempo h√°bil:

- **20.000** exemplos para treino  
- **2.000** exemplos para valida√ß√£o  

### Pr√©-processamento  
Criou-se a coluna `labels`, contendo todos os r√≥tulos em uma √∫nica estrutura multi-label.

---

# 4. Modelo Utilizado

### Arquitetura  
**BERT Base (bert-base-uncased)**

### Framework  
HuggingFace Transformers + PyTorch

### Configura√ß√£o  
- Cabe√ßa multi-label com 6 neur√¥nios  
- Fun√ß√£o de ativa√ß√£o sigmoid  
- Perda: BCEWithLogitsLoss  
- Tokeniza√ß√£o com `max_length=64`, `use_fast=False`, padding din√¢mico  

### Justificativa
O BERT apresenta excelente desempenho em classifica√ß√£o textual e √© um modelo adequado para posteriores an√°lises de robustez e interpretabilidade.

---

# 5. Configura√ß√£o de Treinamento

### Dispositivo  
GPU **MPS** (MacBook Air M4).

### Hiperpar√¢metros  
- √âpocas: **1**  
- Batch size: **8**  
- LR: **2e-5**  
- Otimizador: AdamW  
- Scheduler linear  

### Motiva√ß√£o  
Treinamento parcial, r√°pido, e suficiente para estabelecer um baseline.

---

# 6. Resultados do Treinamento (21/11)

Sa√≠da final:
- Epoch 1/1
- Train loss: 0.0670
- Val loss: 0.0529
- Val F1: 0.7426


### Interpreta√ß√£o  
- O F1 de **0.7426** √© consistente para um treinamento r√°pido.  
- O modelo aprende padr√µes claros de toxicidade mesmo com subset.  
- N√£o h√° sinais de overfitting ou underfitting.  

---

# 7. Valida√ß√£o Adicional do Modelo

Foram testados coment√°rios reais para avaliar coer√™ncia das previs√µes:

```
Texto: "I love this article, very helpful."
Predi√ß√µes: {'toxic': 0.005, 'severe_toxic': 0.002, 'obscene': 0.003, 'threat': 0.002, 'insult': 0.004, 'identity_hate': 0.003}

Texto: "You are stupid and disgusting."
Predi√ß√µes: {'toxic': 0.928, 'severe_toxic': 0.081, 'obscene': 0.636, 'threat': 0.041, 'insult': 0.641, 'identity_hate': 0.109}

Texto: "I'll find you and hurt you."
Predi√ß√µes: {'toxic': 0.567, 'severe_toxic': 0.023, 'obscene': 0.140, 'threat': 0.031, 'insult': 0.252, 'identity_hate': 0.049}

Texto: "Thank you for your support!"
Predi√ß√µes: {'toxic': 0.006, 'severe_toxic': 0.002, 'obscene': 0.003, 'threat': 0.002, 'insult': 0.004, 'identity_hate': 0.003}
````


As previs√µes s√£o coerentes, validando qualitativamente o baseline.

---

# 8. An√°lise de Robustez (28/11)

A robustez avalia quanto o desempenho do modelo se altera quando textos s√£o perturbados, mantendo o significado aproximado.

Foram aplicadas 5 perturba√ß√µes:

1. **Typos** (erros de digita√ß√£o)  
2. **Emojis**  
3. **Eufemismos** (censuras)  
4. **Shuffle leve** (troca m√≠nima de palavras)  
5. **Baseline** (sem perturba√ß√£o)

A avalia√ß√£o foi feita sobre uma amostra de **500** exemplos da valida√ß√£o.

---

## 8.1 Tipos de Perturba√ß√£o

### Typos
Substitui√ß√£o aleat√≥ria de caracteres.  
Impacta o vocabul√°rio, prejudicando o embedding.

### Emojis  
Adi√ß√£o de ‚Äúüò°‚Äù ao final da frase.  
Geralmente n√£o altera o significado.

### Eufemismos  
Censura de palavras t√≥xicas:  
`stupid ‚Üí stu_pid`, `idiot ‚Üí id!ot`, etc.

### Shuffle leve  
Troca de posi√ß√£o entre duas palavras internas.

---

# 9. Resultados de F1 por Perturba√ß√£o

Os valores obtidos foram:

| Perturba√ß√£o | F1 Micro | ŒîF1 | Observa√ß√£o |
|-------------|----------|------|-------------|
| Baseline | **0.74** | ‚Äî | Desempenho original |
| Typos | **0.56** | **-0.18** | Maior queda, sens√≠vel a ru√≠do ortogr√°fico |
| Emojis | **0.74** | +0.00 | Totalmente robusto |
| Eufemismos | **0.70** | -0.04 | Queda moderada |
| Shuffle leve | **0.75** | +0.01 | Efeito quase nulo |

### **Gr√°fico ‚Äî Robustez do Modelo**

![alt text](images/robustez-do-modelo-f1-por-pertubacao.png)

---

# 10. An√°lise Interpretativa

A an√°lise mostra padr√µes importantes:

### Typos  
Foi a perturba√ß√£o mais prejudicial. O modelo depende fortemente do vocabul√°rio limpo ‚Äî pequenas altera√ß√µes j√° reduzem o F1 em quase **18%**, mostrando baixa robustez ortogr√°fica.

### Emojis  
Impacto praticamente nulo. O modelo ignora emojis e mant√©m o desempenho.

### Eufemismos  
Quando palavr√µes s√£o parcialmente mascarados, o modelo perde contexto lexical e cai cerca de **4%**.

### Shuffle leve  
Queda m√≠nima, indicando que o mecanismo de aten√ß√£o do BERT √© robusto a mudan√ßas leves de ordem.

### ‚úî Conclus√£o  
O modelo √© robusto a **perturba√ß√µes superficiais** (emojis, shuffle), mas vulner√°vel a **ru√≠do lexical** (typos, eufemismos), principalmente quando altera a forma da palavra t√≥xica.

---

# 11. Conclus√£o Geral

A etapa de robustez mostrou que:

- O baseline BERT j√° apresenta bom desempenho inicial.  
- O modelo √© fortemente sens√≠vel a perturba√ß√µes que distorcem palavras-chave.  
- Pequenas altera√ß√µes sem√¢nticas (typos, censura) s√£o mais danosas do que altera√ß√µes estruturais (shuffle).  
- O modelo √© altamente robusto √† adi√ß√£o de emojis.

O baseline treinado ser√° usado na pr√≥xima etapa, na investiga√ß√£o de **interpretabilidade** e poss√≠veis **ataques adversariais**.

---

# 12. Refer√™ncias

- Kaggle: Jigsaw Toxic Comment Classification  
- Devlin et al., 2018 ‚Äî BERT  
- HuggingFace Transformers Documentation  

---
